{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from qnet.log_init import initialize_logger\n",
    "initialize_logger()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-02-02 11:27:42,518]\u001B[0m A new study created in memory with name: no-name-764534c5-6289-4bb1-9446-65e30101cf86\u001B[0m\n",
      "[2022-02-02 11:27:42,535    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   211: reducing learning rate of group 0 to 5.1261e-05.\n",
      "Epoch   229: reducing learning rate of group 0 to 1.0252e-05.\n",
      "Epoch   244: reducing learning rate of group 0 to 2.0504e-06.\n",
      "Epoch   260: reducing learning rate of group 0 to 4.1009e-07.\n",
      "Epoch   268: reducing learning rate of group 0 to 8.2018e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:28:27,810    INFO], [train.py:312 - train()], Model training stopped after 287 epochs.\n",
      "[2022-02-02 11:28:27,810    INFO], [train.py:313 - train()], Best testing loss: -4.994\n",
      "\u001B[32m[I 2022-02-02 11:28:27,834]\u001B[0m Trial 0 finished with value: 0.00462675653398037 and parameters: {'lr': 0.0002563054242176569, 'l2_regularization': 0.12336083310014599, 'dropout': 0.19484166770342948, 'batch_size': 0.40572577752847566}. Best is trial 0 with value: 0.00462675653398037.\u001B[0m\n",
      "[2022-02-02 11:28:27,845    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    76: reducing learning rate of group 0 to 1.3523e-04.\n",
      "Epoch    98: reducing learning rate of group 0 to 2.7046e-05.\n",
      "Epoch   112: reducing learning rate of group 0 to 5.4092e-06.\n",
      "Epoch   127: reducing learning rate of group 0 to 1.0818e-06.\n",
      "Epoch   144: reducing learning rate of group 0 to 2.1637e-07.\n",
      "Epoch   154: reducing learning rate of group 0 to 4.3274e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:28:54,744    INFO], [train.py:312 - train()], Model training stopped after 174 epochs.\n",
      "[2022-02-02 11:28:54,745    INFO], [train.py:313 - train()], Best testing loss: -5.34\n",
      "\u001B[32m[I 2022-02-02 11:28:54,768]\u001B[0m Trial 1 finished with value: 0.0064843278378248215 and parameters: {'lr': 0.0006761487705960308, 'l2_regularization': 0.6108175643727799, 'dropout': 0.5477922793563187, 'batch_size': 0.4113947787758992}. Best is trial 0 with value: 0.00462675653398037.\u001B[0m\n",
      "[2022-02-02 11:28:54,779    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 1.1579e-03.\n",
      "Epoch    43: reducing learning rate of group 0 to 2.3158e-04.\n",
      "Epoch    83: reducing learning rate of group 0 to 4.6316e-05.\n",
      "Epoch    97: reducing learning rate of group 0 to 9.2631e-06.\n",
      "Epoch   106: reducing learning rate of group 0 to 1.8526e-06.\n",
      "Epoch   120: reducing learning rate of group 0 to 3.7052e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:29:17,225    INFO], [train.py:312 - train()], Model training stopped after 145 epochs.\n",
      "[2022-02-02 11:29:17,227    INFO], [train.py:313 - train()], Best testing loss: -21.9061\n",
      "\u001B[32m[I 2022-02-02 11:29:17,251]\u001B[0m Trial 2 finished with value: 0.004488676320761442 and parameters: {'lr': 0.005789443169670154, 'l2_regularization': 0.2795447925311849, 'dropout': 0.057424636858373915, 'batch_size': 0.4123218617898649}. Best is trial 2 with value: 0.004488676320761442.\u001B[0m\n",
      "[2022-02-02 11:29:17,262    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    70: reducing learning rate of group 0 to 3.5374e-05.\n",
      "Epoch   107: reducing learning rate of group 0 to 7.0748e-06.\n",
      "Epoch   115: reducing learning rate of group 0 to 1.4150e-06.\n",
      "Epoch   137: reducing learning rate of group 0 to 2.8299e-07.\n",
      "Epoch   145: reducing learning rate of group 0 to 5.6599e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:29:49,843    INFO], [train.py:312 - train()], Model training stopped after 167 epochs.\n",
      "[2022-02-02 11:29:49,844    INFO], [train.py:313 - train()], Best testing loss: -2.2167\n",
      "\u001B[32m[I 2022-02-02 11:29:49,869]\u001B[0m Trial 3 finished with value: 0.006136219948530197 and parameters: {'lr': 0.00017687120857475395, 'l2_regularization': 0.2242656076629609, 'dropout': 0.23200550320689023, 'batch_size': 0.23291443832177688}. Best is trial 2 with value: 0.004488676320761442.\u001B[0m\n",
      "[2022-02-02 11:29:49,879    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   153: reducing learning rate of group 0 to 1.1320e-08.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.8325e-03.\n",
      "Epoch    66: reducing learning rate of group 0 to 3.6649e-04.\n",
      "Epoch    78: reducing learning rate of group 0 to 7.3299e-05.\n",
      "Epoch    87: reducing learning rate of group 0 to 1.4660e-05.\n",
      "Epoch   103: reducing learning rate of group 0 to 2.9320e-06.\n",
      "Epoch   120: reducing learning rate of group 0 to 5.8639e-07.\n",
      "Epoch   128: reducing learning rate of group 0 to 1.1728e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:30:15,497    INFO], [train.py:312 - train()], Model training stopped after 150 epochs.\n",
      "[2022-02-02 11:30:15,498    INFO], [train.py:313 - train()], Best testing loss: -33.9322\n",
      "\u001B[32m[I 2022-02-02 11:30:15,523]\u001B[0m Trial 4 finished with value: 0.005793633405119181 and parameters: {'lr': 0.009162371068563735, 'l2_regularization': 0.2646341659619867, 'dropout': 0.4821817558849704, 'batch_size': 0.24635334104364887}. Best is trial 2 with value: 0.004488676320761442.\u001B[0m\n",
      "[2022-02-02 11:30:15,534    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   136: reducing learning rate of group 0 to 2.3456e-08.\n",
      "Epoch    33: reducing learning rate of group 0 to 1.1588e-03.\n",
      "Epoch    48: reducing learning rate of group 0 to 2.3176e-04.\n",
      "Epoch    67: reducing learning rate of group 0 to 4.6353e-05.\n",
      "Epoch    83: reducing learning rate of group 0 to 9.2706e-06.\n",
      "Epoch    97: reducing learning rate of group 0 to 1.8541e-06.\n",
      "Epoch   105: reducing learning rate of group 0 to 3.7082e-07.\n",
      "Epoch   120: reducing learning rate of group 0 to 7.4164e-08.\n",
      "Epoch   128: reducing learning rate of group 0 to 1.4833e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:31:05,269    INFO], [train.py:312 - train()], Model training stopped after 146 epochs.\n",
      "[2022-02-02 11:31:05,270    INFO], [train.py:313 - train()], Best testing loss: -26.9043\n",
      "\u001B[32m[I 2022-02-02 11:31:05,295]\u001B[0m Trial 5 finished with value: 0.008174114860594273 and parameters: {'lr': 0.0057941008250137985, 'l2_regularization': 0.6341893941988813, 'dropout': 0.5511159727969306, 'batch_size': 0.08090920873310212}. Best is trial 2 with value: 0.004488676320761442.\u001B[0m\n",
      "[2022-02-02 11:31:05,307    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   119: reducing learning rate of group 0 to 1.4875e-04.\n",
      "Epoch   161: reducing learning rate of group 0 to 2.9750e-05.\n",
      "Epoch   174: reducing learning rate of group 0 to 5.9501e-06.\n",
      "Epoch   183: reducing learning rate of group 0 to 1.1900e-06.\n",
      "Epoch   191: reducing learning rate of group 0 to 2.3800e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:31:43,145    INFO], [train.py:312 - train()], Model training stopped after 213 epochs.\n",
      "[2022-02-02 11:31:43,145    INFO], [train.py:313 - train()], Best testing loss: -11.4549\n",
      "\u001B[32m[I 2022-02-02 11:31:43,170]\u001B[0m Trial 6 finished with value: 0.004219419322907925 and parameters: {'lr': 0.0007437580949637173, 'l2_regularization': 0.3562626555018913, 'dropout': 0.456244747723817, 'batch_size': 0.27205869319284065}. Best is trial 6 with value: 0.004219419322907925.\u001B[0m\n",
      "[2022-02-02 11:31:43,182    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   199: reducing learning rate of group 0 to 4.7601e-08.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.3717e-03.\n",
      "Epoch    43: reducing learning rate of group 0 to 2.7433e-04.\n",
      "Epoch    68: reducing learning rate of group 0 to 5.4866e-05.\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0973e-05.\n",
      "Epoch    85: reducing learning rate of group 0 to 2.1947e-06.\n",
      "Epoch    93: reducing learning rate of group 0 to 4.3893e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:32:25,317    INFO], [train.py:312 - train()], Model training stopped after 114 epochs.\n",
      "[2022-02-02 11:32:25,318    INFO], [train.py:313 - train()], Best testing loss: -30.0803\n",
      "\u001B[32m[I 2022-02-02 11:32:25,345]\u001B[0m Trial 7 finished with value: 0.00593440281227231 and parameters: {'lr': 0.006858295057086485, 'l2_regularization': 0.5942092568144093, 'dropout': 0.07089175517124968, 'batch_size': 0.07914372791102244}. Best is trial 6 with value: 0.004219419322907925.\u001B[0m\n",
      "[2022-02-02 11:32:25,357    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    90: reducing learning rate of group 0 to 5.1606e-05.\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0321e-05.\n",
      "Epoch   145: reducing learning rate of group 0 to 2.0642e-06.\n",
      "Epoch   153: reducing learning rate of group 0 to 4.1285e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:32:56,191    INFO], [train.py:312 - train()], Model training stopped after 175 epochs.\n",
      "[2022-02-02 11:32:56,192    INFO], [train.py:313 - train()], Best testing loss: -3.276\n",
      "\u001B[32m[I 2022-02-02 11:32:56,216]\u001B[0m Trial 8 finished with value: 0.0056320130825042725 and parameters: {'lr': 0.00025802875906154056, 'l2_regularization': 0.20942250665158957, 'dropout': 0.1387481016123273, 'batch_size': 0.2863880280354267}. Best is trial 6 with value: 0.004219419322907925.\u001B[0m\n",
      "[2022-02-02 11:32:56,227    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   161: reducing learning rate of group 0 to 8.2569e-08.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.2340e-03.\n",
      "Epoch    26: reducing learning rate of group 0 to 2.4681e-04.\n",
      "Epoch    89: reducing learning rate of group 0 to 4.9362e-05.\n",
      "Epoch   108: reducing learning rate of group 0 to 9.8724e-06.\n",
      "Epoch   127: reducing learning rate of group 0 to 1.9745e-06.\n",
      "Epoch   135: reducing learning rate of group 0 to 3.9490e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:33:23,359    INFO], [train.py:312 - train()], Model training stopped after 157 epochs.\n",
      "[2022-02-02 11:33:23,360    INFO], [train.py:313 - train()], Best testing loss: -21.7225\n",
      "\u001B[32m[I 2022-02-02 11:33:23,385]\u001B[0m Trial 9 finished with value: 0.0041518546640872955 and parameters: {'lr': 0.006170245030722031, 'l2_regularization': 0.3053456910360779, 'dropout': 0.5140429908386654, 'batch_size': 0.2607609006620636}. Best is trial 9 with value: 0.0041518546640872955.\u001B[0m\n",
      "[2022-02-02 11:33:23,408    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   143: reducing learning rate of group 0 to 7.8979e-08.\n",
      "Epoch    97: reducing learning rate of group 0 to 3.5977e-04.\n",
      "Epoch   142: reducing learning rate of group 0 to 7.1954e-05.\n",
      "Epoch   150: reducing learning rate of group 0 to 1.4391e-05.\n",
      "Epoch   158: reducing learning rate of group 0 to 2.8781e-06.\n",
      "Epoch   167: reducing learning rate of group 0 to 5.7563e-07.\n",
      "Epoch   187: reducing learning rate of group 0 to 1.1513e-07.\n",
      "Epoch   195: reducing learning rate of group 0 to 2.3025e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:34:11,198    INFO], [train.py:312 - train()], Model training stopped after 217 epochs.\n",
      "[2022-02-02 11:34:11,199    INFO], [train.py:313 - train()], Best testing loss: -25.7731\n",
      "\u001B[32m[I 2022-02-02 11:34:11,225]\u001B[0m Trial 10 finished with value: 0.005480043590068817 and parameters: {'lr': 0.0017988376946078176, 'l2_regularization': 0.4624985167202671, 'dropout': 0.7204494298856515, 'batch_size': 0.17489217571899301}. Best is trial 9 with value: 0.0041518546640872955.\u001B[0m\n",
      "[2022-02-02 11:34:11,249    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   203: reducing learning rate of group 0 to 4.6050e-09.\n",
      "Epoch     7: reducing learning rate of group 0 to 2.7229e-04.\n",
      "Epoch    27: reducing learning rate of group 0 to 5.4458e-05.\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0892e-05.\n",
      "Epoch    89: reducing learning rate of group 0 to 2.1783e-06.\n",
      "Epoch    97: reducing learning rate of group 0 to 4.3566e-07.\n",
      "Epoch   106: reducing learning rate of group 0 to 8.7133e-08.\n",
      "Epoch   119: reducing learning rate of group 0 to 1.7427e-08.\n",
      "Epoch   127: reducing learning rate of group 0 to 3.4853e-09.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:34:37,700    INFO], [train.py:312 - train()], Model training stopped after 149 epochs.\n",
      "[2022-02-02 11:34:37,701    INFO], [train.py:313 - train()], Best testing loss: -4.1045\n",
      "\u001B[32m[I 2022-02-02 11:34:37,726]\u001B[0m Trial 11 finished with value: 0.006394438911229372 and parameters: {'lr': 0.0013614469133306846, 'l2_regularization': 0.4034310331539797, 'dropout': 0.3255444365770712, 'batch_size': 0.3138158407909939}. Best is trial 9 with value: 0.0041518546640872955.\u001B[0m\n",
      "[2022-02-02 11:34:37,747    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    48: reducing learning rate of group 0 to 4.7941e-04.\n",
      "Epoch    75: reducing learning rate of group 0 to 9.5882e-05.\n",
      "Epoch    86: reducing learning rate of group 0 to 1.9176e-05.\n",
      "Epoch    97: reducing learning rate of group 0 to 3.8353e-06.\n",
      "Epoch   105: reducing learning rate of group 0 to 7.6706e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:34:57,208    INFO], [train.py:312 - train()], Model training stopped after 127 epochs.\n",
      "[2022-02-02 11:34:57,209    INFO], [train.py:313 - train()], Best testing loss: -12.9785\n",
      "\u001B[32m[I 2022-02-02 11:34:57,234]\u001B[0m Trial 12 finished with value: 0.004170259460806847 and parameters: {'lr': 0.0023970514951191135, 'l2_regularization': 0.3844386710675475, 'dropout': 0.41184432238808005, 'batch_size': 0.3306595698920073}. Best is trial 9 with value: 0.0041518546640872955.\u001B[0m\n",
      "[2022-02-02 11:34:57,255    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   113: reducing learning rate of group 0 to 1.5341e-07.\n",
      "Epoch    62: reducing learning rate of group 0 to 5.8995e-04.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.1799e-04.\n",
      "Epoch    91: reducing learning rate of group 0 to 2.3598e-05.\n",
      "Epoch   107: reducing learning rate of group 0 to 4.7196e-06.\n",
      "Epoch   134: reducing learning rate of group 0 to 9.4391e-07.\n",
      "Epoch   142: reducing learning rate of group 0 to 1.8878e-07.\n",
      "Epoch   169: reducing learning rate of group 0 to 3.7756e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:35:27,302    INFO], [train.py:312 - train()], Model training stopped after 195 epochs.\n",
      "[2022-02-02 11:35:27,303    INFO], [train.py:313 - train()], Best testing loss: -17.1935\n",
      "\u001B[32m[I 2022-02-02 11:35:27,327]\u001B[0m Trial 13 finished with value: 0.004727588500827551 and parameters: {'lr': 0.0029497261976077363, 'l2_regularization': 0.4950498333318659, 'dropout': 0.36183769845217906, 'batch_size': 0.3510785493910858}. Best is trial 9 with value: 0.0041518546640872955.\u001B[0m\n",
      "[2022-02-02 11:35:27,348    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   180: reducing learning rate of group 0 to 7.5513e-09.\n",
      "Epoch    55: reducing learning rate of group 0 to 6.1738e-04.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.2348e-04.\n",
      "Epoch    77: reducing learning rate of group 0 to 2.4695e-05.\n",
      "Epoch    87: reducing learning rate of group 0 to 4.9391e-06.\n",
      "Epoch    96: reducing learning rate of group 0 to 9.8782e-07.\n",
      "Epoch   109: reducing learning rate of group 0 to 1.9756e-07.\n",
      "Epoch   117: reducing learning rate of group 0 to 3.9513e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:36:00,596    INFO], [train.py:312 - train()], Model training stopped after 139 epochs.\n",
      "[2022-02-02 11:36:00,596    INFO], [train.py:313 - train()], Best testing loss: -23.1747\n",
      "\u001B[32m[I 2022-02-02 11:36:00,622]\u001B[0m Trial 14 finished with value: 0.005945312790572643 and parameters: {'lr': 0.003086924790969957, 'l2_regularization': 0.7560027124581504, 'dropout': 0.6891311139491076, 'batch_size': 0.15954062347251963}. Best is trial 9 with value: 0.0041518546640872955.\u001B[0m\n",
      "[2022-02-02 11:36:00,644    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   125: reducing learning rate of group 0 to 7.9025e-09.\n",
      "Epoch    46: reducing learning rate of group 0 to 5.9055e-04.\n",
      "Epoch   144: reducing learning rate of group 0 to 1.1811e-04.\n",
      "Epoch   162: reducing learning rate of group 0 to 2.3622e-05.\n",
      "Epoch   179: reducing learning rate of group 0 to 4.7244e-06.\n",
      "Epoch   203: reducing learning rate of group 0 to 9.4488e-07.\n",
      "Epoch   211: reducing learning rate of group 0 to 1.8898e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:36:36,894    INFO], [train.py:312 - train()], Model training stopped after 233 epochs.\n",
      "[2022-02-02 11:36:36,895    INFO], [train.py:313 - train()], Best testing loss: -18.0909\n",
      "\u001B[32m[I 2022-02-02 11:36:36,919]\u001B[0m Trial 15 finished with value: 0.005435747094452381 and parameters: {'lr': 0.002952763998986852, 'l2_regularization': 0.3522702521995436, 'dropout': 0.6146273286985324, 'batch_size': 0.4681899080695764}. Best is trial 9 with value: 0.0041518546640872955.\u001B[0m\n",
      "[2022-02-02 11:36:36,941    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   219: reducing learning rate of group 0 to 3.7795e-08.\n",
      "Epoch    22: reducing learning rate of group 0 to 8.1255e-04.\n",
      "Epoch   161: reducing learning rate of group 0 to 1.6251e-04.\n",
      "Epoch   172: reducing learning rate of group 0 to 3.2502e-05.\n",
      "Epoch   183: reducing learning rate of group 0 to 6.5004e-06.\n",
      "Epoch   191: reducing learning rate of group 0 to 1.3001e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:37:09,602    INFO], [train.py:312 - train()], Model training stopped after 213 epochs.\n",
      "[2022-02-02 11:37:09,603    INFO], [train.py:313 - train()], Best testing loss: -20.7295\n",
      "\u001B[32m[I 2022-02-02 11:37:09,627]\u001B[0m Trial 16 finished with value: 0.00357465585693717 and parameters: {'lr': 0.004062767270777477, 'l2_regularization': 0.10804886260917249, 'dropout': 0.29933554666475337, 'batch_size': 0.3374401770011024}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:37:09,649    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   199: reducing learning rate of group 0 to 2.6002e-07.\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0093e-03.\n",
      "Epoch    79: reducing learning rate of group 0 to 2.0186e-04.\n",
      "Epoch    96: reducing learning rate of group 0 to 4.0372e-05.\n",
      "Epoch   117: reducing learning rate of group 0 to 8.0744e-06.\n",
      "Epoch   136: reducing learning rate of group 0 to 1.6149e-06.\n",
      "Epoch   157: reducing learning rate of group 0 to 3.2298e-07.\n",
      "Epoch   166: reducing learning rate of group 0 to 6.4596e-08.\n",
      "Epoch   180: reducing learning rate of group 0 to 1.2919e-08.\n",
      "Epoch   188: reducing learning rate of group 0 to 2.5838e-09.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:37:49,565    INFO], [train.py:312 - train()], Model training stopped after 210 epochs.\n",
      "[2022-02-02 11:37:49,565    INFO], [train.py:313 - train()], Best testing loss: -37.7994\n",
      "\u001B[32m[I 2022-02-02 11:37:49,590]\u001B[0m Trial 17 finished with value: 0.004380007740110159 and parameters: {'lr': 0.0050465247689340815, 'l2_regularization': 0.1592410027627083, 'dropout': 0.2875242081532282, 'batch_size': 0.20030506701964612}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:37:49,613    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   108: reducing learning rate of group 0 to 2.1512e-04.\n",
      "Epoch   151: reducing learning rate of group 0 to 4.3024e-05.\n",
      "Epoch   182: reducing learning rate of group 0 to 8.6047e-06.\n",
      "Epoch   190: reducing learning rate of group 0 to 1.7209e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:38:17,741    INFO], [train.py:312 - train()], Model training stopped after 212 epochs.\n",
      "[2022-02-02 11:38:17,741    INFO], [train.py:313 - train()], Best testing loss: -8.0011\n",
      "\u001B[32m[I 2022-02-02 11:38:17,766]\u001B[0m Trial 18 finished with value: 0.004082098603248596 and parameters: {'lr': 0.0010755903323850166, 'l2_regularization': 0.10851525822218525, 'dropout': 0.26779295426455013, 'batch_size': 0.4907203352076571}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:38:17,788    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   198: reducing learning rate of group 0 to 3.4419e-07.\n",
      "Epoch   186: reducing learning rate of group 0 to 9.3340e-05.\n",
      "Epoch   228: reducing learning rate of group 0 to 1.8668e-05.\n",
      "Epoch   236: reducing learning rate of group 0 to 3.7336e-06.\n",
      "Epoch   257: reducing learning rate of group 0 to 7.4672e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:38:54,815    INFO], [train.py:312 - train()], Model training stopped after 278 epochs.\n",
      "[2022-02-02 11:38:54,816    INFO], [train.py:313 - train()], Best testing loss: -5.4626\n",
      "\u001B[32m[I 2022-02-02 11:38:54,841]\u001B[0m Trial 19 finished with value: 0.0048496234230697155 and parameters: {'lr': 0.0004667019870960094, 'l2_regularization': 0.1160002310021281, 'dropout': 0.2608860427181459, 'batch_size': 0.49967738127103456}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:38:54,863    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   156: reducing learning rate of group 0 to 2.2661e-04.\n",
      "Epoch   168: reducing learning rate of group 0 to 4.5322e-05.\n",
      "Epoch   202: reducing learning rate of group 0 to 9.0645e-06.\n",
      "Epoch   210: reducing learning rate of group 0 to 1.8129e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:39:30,566    INFO], [train.py:312 - train()], Model training stopped after 232 epochs.\n",
      "[2022-02-02 11:39:30,566    INFO], [train.py:313 - train()], Best testing loss: -15.8059\n",
      "\u001B[32m[I 2022-02-02 11:39:30,590]\u001B[0m Trial 20 finished with value: 0.0038297628052532673 and parameters: {'lr': 0.001133059274253488, 'l2_regularization': 0.2039803951549246, 'dropout': 0.15346991710272676, 'batch_size': 0.3773958074339001}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:39:30,612    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   218: reducing learning rate of group 0 to 3.6258e-07.\n",
      "Epoch   130: reducing learning rate of group 0 to 2.1029e-04.\n",
      "Epoch   144: reducing learning rate of group 0 to 4.2057e-05.\n",
      "Epoch   164: reducing learning rate of group 0 to 8.4115e-06.\n",
      "Epoch   177: reducing learning rate of group 0 to 1.6823e-06.\n",
      "Epoch   185: reducing learning rate of group 0 to 3.3646e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:40:13,057    INFO], [train.py:312 - train()], Model training stopped after 254 epochs.\n",
      "[2022-02-02 11:40:13,058    INFO], [train.py:313 - train()], Best testing loss: -12.7772\n",
      "\u001B[32m[I 2022-02-02 11:40:13,087]\u001B[0m Trial 21 finished with value: 0.003699847497045994 and parameters: {'lr': 0.0010514368526347237, 'l2_regularization': 0.18379685526027698, 'dropout': 0.16409282047490834, 'batch_size': 0.3724818644192364}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:40:13,115    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   139: reducing learning rate of group 0 to 1.1948e-04.\n",
      "Epoch   158: reducing learning rate of group 0 to 2.3895e-05.\n",
      "Epoch   179: reducing learning rate of group 0 to 4.7790e-06.\n",
      "Epoch   198: reducing learning rate of group 0 to 9.5580e-07.\n",
      "Epoch   206: reducing learning rate of group 0 to 1.9116e-07.\n",
      "Epoch   233: reducing learning rate of group 0 to 3.8232e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:40:52,409    INFO], [train.py:312 - train()], Model training stopped after 253 epochs.\n",
      "[2022-02-02 11:40:52,410    INFO], [train.py:313 - train()], Best testing loss: -7.9105\n",
      "\u001B[32m[I 2022-02-02 11:40:52,434]\u001B[0m Trial 22 finished with value: 0.00446030218154192 and parameters: {'lr': 0.0005973756811276658, 'l2_regularization': 0.1831792112163512, 'dropout': 0.16545728715960736, 'batch_size': 0.37151941645016595}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:40:52,456    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    58: reducing learning rate of group 0 to 3.0649e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 6.1298e-05.\n",
      "Epoch   134: reducing learning rate of group 0 to 1.2260e-05.\n",
      "Epoch   144: reducing learning rate of group 0 to 2.4519e-06.\n",
      "Epoch   155: reducing learning rate of group 0 to 4.9038e-07.\n",
      "Epoch   163: reducing learning rate of group 0 to 9.8076e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:41:20,687    INFO], [train.py:312 - train()], Model training stopped after 183 epochs.\n",
      "[2022-02-02 11:41:20,688    INFO], [train.py:313 - train()], Best testing loss: -10.5715\n",
      "\u001B[32m[I 2022-02-02 11:41:20,712]\u001B[0m Trial 23 finished with value: 0.00376907573081553 and parameters: {'lr': 0.0015324397474291553, 'l2_regularization': 0.22518390204276317, 'dropout': 0.12104146135146643, 'batch_size': 0.3824873915512811}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:41:20,734    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    37: reducing learning rate of group 0 to 3.5810e-04.\n",
      "Epoch    88: reducing learning rate of group 0 to 7.1619e-05.\n",
      "Epoch   108: reducing learning rate of group 0 to 1.4324e-05.\n",
      "Epoch   120: reducing learning rate of group 0 to 2.8648e-06.\n",
      "Epoch   128: reducing learning rate of group 0 to 5.7296e-07.\n",
      "Epoch   136: reducing learning rate of group 0 to 1.1459e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:41:48,416    INFO], [train.py:312 - train()], Model training stopped after 158 epochs.\n",
      "[2022-02-02 11:41:48,416    INFO], [train.py:313 - train()], Best testing loss: -12.2513\n",
      "\u001B[32m[I 2022-02-02 11:41:48,441]\u001B[0m Trial 24 finished with value: 0.004126229323446751 and parameters: {'lr': 0.001790485449664244, 'l2_regularization': 0.2401563576929556, 'dropout': 0.09175905855537458, 'batch_size': 0.31397435853944117}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:41:48,464    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   144: reducing learning rate of group 0 to 2.2918e-08.\n",
      "Epoch    66: reducing learning rate of group 0 to 8.2759e-05.\n",
      "Epoch    84: reducing learning rate of group 0 to 1.6552e-05.\n",
      "Epoch    95: reducing learning rate of group 0 to 3.3104e-06.\n",
      "Epoch   112: reducing learning rate of group 0 to 6.6207e-07.\n",
      "Epoch   120: reducing learning rate of group 0 to 1.3241e-07.\n",
      "Epoch   129: reducing learning rate of group 0 to 2.6483e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:42:11,545    INFO], [train.py:312 - train()], Model training stopped after 148 epochs.\n",
      "[2022-02-02 11:42:11,546    INFO], [train.py:313 - train()], Best testing loss: -2.9198\n",
      "\u001B[32m[I 2022-02-02 11:42:11,570]\u001B[0m Trial 25 finished with value: 0.005409721285104752 and parameters: {'lr': 0.0004137962370523987, 'l2_regularization': 0.1514834605506647, 'dropout': 0.20995588075956262, 'batch_size': 0.4534599179625128}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:42:11,592    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    46: reducing learning rate of group 0 to 8.2401e-04.\n",
      "Epoch    54: reducing learning rate of group 0 to 1.6480e-04.\n",
      "Epoch   103: reducing learning rate of group 0 to 3.2961e-05.\n",
      "Epoch   117: reducing learning rate of group 0 to 6.5921e-06.\n",
      "Epoch   131: reducing learning rate of group 0 to 1.3184e-06.\n",
      "Epoch   139: reducing learning rate of group 0 to 2.6368e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:42:37,614    INFO], [train.py:312 - train()], Model training stopped after 161 epochs.\n",
      "[2022-02-02 11:42:37,615    INFO], [train.py:313 - train()], Best testing loss: -19.4118\n",
      "\u001B[32m[I 2022-02-02 11:42:37,639]\u001B[0m Trial 26 finished with value: 0.004082259722054005 and parameters: {'lr': 0.004120067555290669, 'l2_regularization': 0.30838683334460787, 'dropout': 0.12996035945496237, 'batch_size': 0.4286308210715826}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:42:37,663    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   147: reducing learning rate of group 0 to 5.2737e-08.\n",
      "Epoch   159: reducing learning rate of group 0 to 3.1220e-04.\n",
      "Epoch   178: reducing learning rate of group 0 to 6.2441e-05.\n",
      "Epoch   186: reducing learning rate of group 0 to 1.2488e-05.\n",
      "Epoch   194: reducing learning rate of group 0 to 2.4976e-06.\n",
      "Epoch   227: reducing learning rate of group 0 to 4.9953e-07.\n",
      "Epoch   239: reducing learning rate of group 0 to 9.9905e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:43:19,623    INFO], [train.py:312 - train()], Model training stopped after 266 epochs.\n",
      "[2022-02-02 11:43:19,624    INFO], [train.py:313 - train()], Best testing loss: -21.7235\n",
      "\u001B[32m[I 2022-02-02 11:43:19,648]\u001B[0m Trial 27 finished with value: 0.003774201963096857 and parameters: {'lr': 0.0015610158698211684, 'l2_regularization': 0.10082398873779216, 'dropout': 0.33221003722635634, 'batch_size': 0.3812737522020734}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:43:19,669    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   155: reducing learning rate of group 0 to 1.7640e-04.\n",
      "Epoch   183: reducing learning rate of group 0 to 3.5279e-05.\n",
      "Epoch   195: reducing learning rate of group 0 to 7.0558e-06.\n",
      "Epoch   209: reducing learning rate of group 0 to 1.4112e-06.\n",
      "Epoch   217: reducing learning rate of group 0 to 2.8223e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:43:56,482    INFO], [train.py:312 - train()], Model training stopped after 239 epochs.\n",
      "[2022-02-02 11:43:56,482    INFO], [train.py:313 - train()], Best testing loss: -12.6556\n",
      "\u001B[32m[I 2022-02-02 11:43:56,507]\u001B[0m Trial 28 finished with value: 0.003854514565318823 and parameters: {'lr': 0.0008819792304818177, 'l2_regularization': 0.16763854998116695, 'dropout': 0.11088163162972398, 'batch_size': 0.3392432823706763}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:43:56,530    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   225: reducing learning rate of group 0 to 5.6447e-08.\n",
      "Epoch     6: reducing learning rate of group 0 to 2.1641e-05.\n",
      "Epoch    14: reducing learning rate of group 0 to 4.3283e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:44:02,952    INFO], [train.py:312 - train()], Model training stopped after 36 epochs.\n",
      "[2022-02-02 11:44:02,953    INFO], [train.py:313 - train()], Best testing loss: 4.4067\n",
      "\u001B[32m[I 2022-02-02 11:44:02,977]\u001B[0m Trial 29 finished with value: 0.14599384367465973 and parameters: {'lr': 0.00010820695794287766, 'l2_regularization': 0.5350129937754174, 'dropout': 0.19874075492781898, 'batch_size': 0.2922442765161768}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:44:03,001    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    22: reducing learning rate of group 0 to 8.6566e-07.\n",
      "Epoch    17: reducing learning rate of group 0 to 4.3589e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 8.7179e-05.\n",
      "Epoch    77: reducing learning rate of group 0 to 1.7436e-05.\n",
      "Epoch    89: reducing learning rate of group 0 to 3.4872e-06.\n",
      "Epoch    97: reducing learning rate of group 0 to 6.9743e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:44:21,608    INFO], [train.py:312 - train()], Model training stopped after 119 epochs.\n",
      "[2022-02-02 11:44:21,608    INFO], [train.py:313 - train()], Best testing loss: -6.3782\n",
      "\u001B[32m[I 2022-02-02 11:44:21,633]\u001B[0m Trial 30 finished with value: 0.0067076836712658405 and parameters: {'lr': 0.002179470665848248, 'l2_regularization': 0.7821271649707545, 'dropout': 0.3830605886014637, 'batch_size': 0.4375451838747511}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:44:21,656    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   105: reducing learning rate of group 0 to 1.3949e-07.\n",
      "Epoch   125: reducing learning rate of group 0 to 2.4819e-04.\n",
      "Epoch   137: reducing learning rate of group 0 to 4.9637e-05.\n",
      "Epoch   146: reducing learning rate of group 0 to 9.9274e-06.\n",
      "Epoch   170: reducing learning rate of group 0 to 1.9855e-06.\n",
      "Epoch   185: reducing learning rate of group 0 to 3.9710e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:44:55,027    INFO], [train.py:312 - train()], Model training stopped after 215 epochs.\n",
      "[2022-02-02 11:44:55,028    INFO], [train.py:313 - train()], Best testing loss: -14.4444\n",
      "\u001B[32m[I 2022-02-02 11:44:55,054]\u001B[0m Trial 31 finished with value: 0.00401438120752573 and parameters: {'lr': 0.0012409253863175991, 'l2_regularization': 0.1091645417375017, 'dropout': 0.30321784320742434, 'batch_size': 0.38049454888314393}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:44:55,084    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    73: reducing learning rate of group 0 to 3.2010e-04.\n",
      "Epoch   162: reducing learning rate of group 0 to 6.4021e-05.\n",
      "Epoch   181: reducing learning rate of group 0 to 1.2804e-05.\n",
      "Epoch   189: reducing learning rate of group 0 to 2.5608e-06.\n",
      "Epoch   197: reducing learning rate of group 0 to 5.1217e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:45:29,041    INFO], [train.py:312 - train()], Model training stopped after 219 epochs.\n",
      "[2022-02-02 11:45:29,042    INFO], [train.py:313 - train()], Best testing loss: -13.8726\n",
      "\u001B[32m[I 2022-02-02 11:45:29,066]\u001B[0m Trial 32 finished with value: 0.0038518731016665697 and parameters: {'lr': 0.0016005236631716816, 'l2_regularization': 0.1504147926702844, 'dropout': 0.3390747090952615, 'batch_size': 0.3966765359380722}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:45:29,089    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   205: reducing learning rate of group 0 to 1.0243e-07.\n",
      "Epoch    76: reducing learning rate of group 0 to 7.7462e-04.\n",
      "Epoch    84: reducing learning rate of group 0 to 1.5492e-04.\n",
      "Epoch   107: reducing learning rate of group 0 to 3.0985e-05.\n",
      "Epoch   122: reducing learning rate of group 0 to 6.1970e-06.\n",
      "Epoch   138: reducing learning rate of group 0 to 1.2394e-06.\n",
      "Epoch   146: reducing learning rate of group 0 to 2.4788e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:45:55,338    INFO], [train.py:312 - train()], Model training stopped after 168 epochs.\n",
      "[2022-02-02 11:45:55,339    INFO], [train.py:313 - train()], Best testing loss: -25.0506\n",
      "\u001B[32m[I 2022-02-02 11:45:55,363]\u001B[0m Trial 33 finished with value: 0.004307162947952747 and parameters: {'lr': 0.003873096894860345, 'l2_regularization': 0.2661480977109105, 'dropout': 0.24698442569287465, 'batch_size': 0.4106640750055394}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:45:55,385    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   154: reducing learning rate of group 0 to 4.9576e-08.\n",
      "Epoch   186: reducing learning rate of group 0 to 1.7225e-04.\n",
      "Epoch   222: reducing learning rate of group 0 to 3.4451e-05.\n",
      "Epoch   237: reducing learning rate of group 0 to 6.8902e-06.\n",
      "Epoch   245: reducing learning rate of group 0 to 1.3780e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:46:36,263    INFO], [train.py:312 - train()], Model training stopped after 266 epochs.\n",
      "[2022-02-02 11:46:36,264    INFO], [train.py:313 - train()], Best testing loss: -14.646\n",
      "\u001B[32m[I 2022-02-02 11:46:36,288]\u001B[0m Trial 34 finished with value: 0.003925775643438101 and parameters: {'lr': 0.0008612708899734722, 'l2_regularization': 0.10049761935660734, 'dropout': 0.4134799981184881, 'batch_size': 0.3521632438598038}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:46:36,311    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   225: reducing learning rate of group 0 to 1.0421e-04.\n",
      "Epoch   260: reducing learning rate of group 0 to 2.0843e-05.\n",
      "Epoch   274: reducing learning rate of group 0 to 4.1685e-06.\n",
      "Epoch   282: reducing learning rate of group 0 to 8.3371e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:47:22,928    INFO], [train.py:312 - train()], Model training stopped after 304 epochs.\n",
      "[2022-02-02 11:47:22,929    INFO], [train.py:313 - train()], Best testing loss: -10.5848\n",
      "\u001B[32m[I 2022-02-02 11:47:22,954]\u001B[0m Trial 35 finished with value: 0.0041528064757585526 and parameters: {'lr': 0.0005210660578779035, 'l2_regularization': 0.20514495055040793, 'dropout': 0.18265879144348915, 'batch_size': 0.3947389920516087}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:47:22,978    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   290: reducing learning rate of group 0 to 1.6674e-07.\n",
      "Epoch    37: reducing learning rate of group 0 to 1.7195e-03.\n",
      "Epoch    45: reducing learning rate of group 0 to 3.4389e-04.\n",
      "Epoch    76: reducing learning rate of group 0 to 6.8779e-05.\n",
      "Epoch    84: reducing learning rate of group 0 to 1.3756e-05.\n",
      "Epoch    98: reducing learning rate of group 0 to 2.7511e-06.\n",
      "Epoch   106: reducing learning rate of group 0 to 5.5023e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:47:42,543    INFO], [train.py:312 - train()], Model training stopped after 127 epochs.\n",
      "[2022-02-02 11:47:42,544    INFO], [train.py:313 - train()], Best testing loss: -30.9226\n",
      "\u001B[32m[I 2022-02-02 11:47:42,568]\u001B[0m Trial 36 finished with value: 0.003915297798812389 and parameters: {'lr': 0.008597340446440158, 'l2_regularization': 0.1469606897758617, 'dropout': 0.2288442833720169, 'batch_size': 0.36490675659116767}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:47:42,591    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   124: reducing learning rate of group 0 to 7.1238e-05.\n",
      "Epoch   139: reducing learning rate of group 0 to 1.4248e-05.\n",
      "Epoch   175: reducing learning rate of group 0 to 2.8495e-06.\n",
      "Epoch   193: reducing learning rate of group 0 to 5.6990e-07.\n",
      "Epoch   201: reducing learning rate of group 0 to 1.1398e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:48:21,889    INFO], [train.py:312 - train()], Model training stopped after 223 epochs.\n",
      "[2022-02-02 11:48:21,890    INFO], [train.py:313 - train()], Best testing loss: -5.7617\n",
      "\u001B[32m[I 2022-02-02 11:48:21,915]\u001B[0m Trial 37 finished with value: 0.005180362146347761 and parameters: {'lr': 0.0003561903366318149, 'l2_regularization': 0.2397943719159144, 'dropout': 0.45748044529449183, 'batch_size': 0.3136188415519273}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:48:21,937    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   209: reducing learning rate of group 0 to 2.2796e-08.\n",
      "Epoch    98: reducing learning rate of group 0 to 1.3530e-04.\n",
      "Epoch   151: reducing learning rate of group 0 to 2.7060e-05.\n",
      "Epoch   162: reducing learning rate of group 0 to 5.4121e-06.\n",
      "Epoch   170: reducing learning rate of group 0 to 1.0824e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:48:51,744    INFO], [train.py:312 - train()], Model training stopped after 192 epochs.\n",
      "[2022-02-02 11:48:51,745    INFO], [train.py:313 - train()], Best testing loss: -6.9191\n",
      "\u001B[32m[I 2022-02-02 11:48:51,771]\u001B[0m Trial 38 finished with value: 0.005010473076254129 and parameters: {'lr': 0.0006765065029217807, 'l2_regularization': 0.30264029061328046, 'dropout': 0.06560577301201684, 'batch_size': 0.4270579456591081}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:48:51,794    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   178: reducing learning rate of group 0 to 2.1648e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 4.7618e-04.\n",
      "Epoch    94: reducing learning rate of group 0 to 9.5237e-05.\n",
      "Epoch   115: reducing learning rate of group 0 to 1.9047e-05.\n",
      "Epoch   125: reducing learning rate of group 0 to 3.8095e-06.\n",
      "Epoch   133: reducing learning rate of group 0 to 7.6189e-07.\n",
      "Epoch   141: reducing learning rate of group 0 to 1.5238e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:49:26,674    INFO], [train.py:312 - train()], Model training stopped after 162 epochs.\n",
      "[2022-02-02 11:49:26,675    INFO], [train.py:313 - train()], Best testing loss: -24.0981\n",
      "\u001B[32m[I 2022-02-02 11:49:26,704]\u001B[0m Trial 39 finished with value: 0.004073551390320063 and parameters: {'lr': 0.0023809202739351496, 'l2_regularization': 0.18184171816168396, 'dropout': 0.1855681022544607, 'batch_size': 0.22432227792089413}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:49:26,729    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    66: reducing learning rate of group 0 to 2.8882e-04.\n",
      "Epoch   118: reducing learning rate of group 0 to 5.7764e-05.\n",
      "Epoch   129: reducing learning rate of group 0 to 1.1553e-05.\n",
      "Epoch   137: reducing learning rate of group 0 to 2.3105e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:49:52,292    INFO], [train.py:312 - train()], Model training stopped after 159 epochs.\n",
      "[2022-02-02 11:49:52,292    INFO], [train.py:313 - train()], Best testing loss: -10.8806\n",
      "\u001B[32m[I 2022-02-02 11:49:52,317]\u001B[0m Trial 40 finished with value: 0.004101288039237261 and parameters: {'lr': 0.0014440885535116903, 'l2_regularization': 0.2222482203965538, 'dropout': 0.31869521643407905, 'batch_size': 0.33366103466659125}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:49:52,340    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   145: reducing learning rate of group 0 to 4.6211e-07.\n",
      "Epoch    35: reducing learning rate of group 0 to 2.1141e-04.\n",
      "Epoch   178: reducing learning rate of group 0 to 4.2282e-05.\n",
      "Epoch   195: reducing learning rate of group 0 to 8.4565e-06.\n",
      "Epoch   206: reducing learning rate of group 0 to 1.6913e-06.\n",
      "Epoch   214: reducing learning rate of group 0 to 3.3826e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:50:28,554    INFO], [train.py:312 - train()], Model training stopped after 235 epochs.\n",
      "[2022-02-02 11:50:28,555    INFO], [train.py:313 - train()], Best testing loss: -6.9897\n",
      "\u001B[32m[I 2022-02-02 11:50:28,578]\u001B[0m Trial 41 finished with value: 0.004607221111655235 and parameters: {'lr': 0.0010570620583127937, 'l2_regularization': 0.20043287948399158, 'dropout': 0.1481051253161141, 'batch_size': 0.3763301657116257}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:50:28,600    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   197: reducing learning rate of group 0 to 2.3418e-04.\n",
      "Epoch   210: reducing learning rate of group 0 to 4.6836e-05.\n",
      "Epoch   229: reducing learning rate of group 0 to 9.3672e-06.\n",
      "Epoch   246: reducing learning rate of group 0 to 1.8734e-06.\n",
      "Epoch   254: reducing learning rate of group 0 to 3.7469e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:51:17,685    INFO], [train.py:312 - train()], Model training stopped after 276 epochs.\n",
      "[2022-02-02 11:51:17,686    INFO], [train.py:313 - train()], Best testing loss: -19.8378\n",
      "\u001B[32m[I 2022-02-02 11:51:17,714]\u001B[0m Trial 42 finished with value: 0.003798888996243477 and parameters: {'lr': 0.0011709019526855195, 'l2_regularization': 0.1323050499265177, 'dropout': 0.10453062201487248, 'batch_size': 0.39794588140646026}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:51:17,738    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    54: reducing learning rate of group 0 to 3.8339e-04.\n",
      "Epoch   136: reducing learning rate of group 0 to 7.6679e-05.\n",
      "Epoch   152: reducing learning rate of group 0 to 1.5336e-05.\n",
      "Epoch   160: reducing learning rate of group 0 to 3.0672e-06.\n",
      "Epoch   177: reducing learning rate of group 0 to 6.1343e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:51:55,076    INFO], [train.py:312 - train()], Model training stopped after 220 epochs.\n",
      "[2022-02-02 11:51:55,077    INFO], [train.py:313 - train()], Best testing loss: -13.5199\n",
      "\u001B[32m[I 2022-02-02 11:51:55,101]\u001B[0m Trial 43 finished with value: 0.003981518559157848 and parameters: {'lr': 0.0019169695165852832, 'l2_regularization': 0.13691624224331136, 'dropout': 0.10430271786776175, 'batch_size': 0.40451765640377324}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:51:55,124    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   148: reducing learning rate of group 0 to 1.4410e-04.\n",
      "Epoch   219: reducing learning rate of group 0 to 2.8819e-05.\n",
      "Epoch   233: reducing learning rate of group 0 to 5.7639e-06.\n",
      "Epoch   241: reducing learning rate of group 0 to 1.1528e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:52:35,976    INFO], [train.py:312 - train()], Model training stopped after 262 epochs.\n",
      "[2022-02-02 11:52:35,977    INFO], [train.py:313 - train()], Best testing loss: -10.5304\n",
      "\u001B[32m[I 2022-02-02 11:52:36,001]\u001B[0m Trial 44 finished with value: 0.004188769496977329 and parameters: {'lr': 0.0007204815229307571, 'l2_regularization': 0.1329115936962652, 'dropout': 0.05492734289338702, 'batch_size': 0.44909312506572824}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:52:36,024    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 1.8673e-04.\n",
      "Epoch    73: reducing learning rate of group 0 to 3.7347e-05.\n",
      "Epoch    81: reducing learning rate of group 0 to 7.4694e-06.\n",
      "Epoch    96: reducing learning rate of group 0 to 1.4939e-06.\n",
      "Epoch   104: reducing learning rate of group 0 to 2.9878e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:52:58,075    INFO], [train.py:312 - train()], Model training stopped after 126 epochs.\n",
      "[2022-02-02 11:52:58,075    INFO], [train.py:313 - train()], Best testing loss: -5.9338\n",
      "\u001B[32m[I 2022-02-02 11:52:58,101]\u001B[0m Trial 45 finished with value: 0.005279266741126776 and parameters: {'lr': 0.0009336737102076343, 'l2_regularization': 0.2525820020092925, 'dropout': 0.2240220453206504, 'batch_size': 0.3026843508364596}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:52:58,123    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   112: reducing learning rate of group 0 to 5.9755e-08.\n",
      "Epoch    99: reducing learning rate of group 0 to 2.8694e-04.\n",
      "Epoch   120: reducing learning rate of group 0 to 5.7387e-05.\n",
      "Epoch   131: reducing learning rate of group 0 to 1.1477e-05.\n",
      "Epoch   143: reducing learning rate of group 0 to 2.2955e-06.\n",
      "Epoch   151: reducing learning rate of group 0 to 4.5910e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:53:28,149    INFO], [train.py:312 - train()], Model training stopped after 173 epochs.\n",
      "[2022-02-02 11:53:28,149    INFO], [train.py:313 - train()], Best testing loss: -17.9568\n",
      "\u001B[32m[I 2022-02-02 11:53:28,174]\u001B[0m Trial 46 finished with value: 0.0040436116978526115 and parameters: {'lr': 0.0014346853026555321, 'l2_regularization': 0.17589489724435703, 'dropout': 0.08648533684117034, 'batch_size': 0.27259863244018495}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:53:28,197    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   159: reducing learning rate of group 0 to 9.1820e-08.\n",
      "Epoch    44: reducing learning rate of group 0 to 7.9676e-04.\n",
      "Epoch    66: reducing learning rate of group 0 to 1.5935e-04.\n",
      "Epoch    78: reducing learning rate of group 0 to 3.1870e-05.\n",
      "Epoch    86: reducing learning rate of group 0 to 6.3741e-06.\n",
      "Epoch    99: reducing learning rate of group 0 to 1.2748e-06.\n",
      "Epoch   110: reducing learning rate of group 0 to 2.5496e-07.\n",
      "Epoch   118: reducing learning rate of group 0 to 5.0992e-08.\n",
      "Epoch   126: reducing learning rate of group 0 to 1.0198e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:54:11,585    INFO], [train.py:312 - train()], Model training stopped after 155 epochs.\n",
      "[2022-02-02 11:54:11,586    INFO], [train.py:313 - train()], Best testing loss: -27.9412\n",
      "\u001B[32m[I 2022-02-02 11:54:11,611]\u001B[0m Trial 47 finished with value: 0.0046375226229429245 and parameters: {'lr': 0.003983786787303936, 'l2_regularization': 0.6572010448319374, 'dropout': 0.11643456236055331, 'batch_size': 0.11024214022785839}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:54:11,633    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    52: reducing learning rate of group 0 to 4.0316e-04.\n",
      "Epoch   176: reducing learning rate of group 0 to 8.0632e-05.\n",
      "Epoch   184: reducing learning rate of group 0 to 1.6126e-05.\n",
      "Epoch   199: reducing learning rate of group 0 to 3.2253e-06.\n",
      "Epoch   208: reducing learning rate of group 0 to 6.4506e-07.\n",
      "Epoch   216: reducing learning rate of group 0 to 1.2901e-07.\n",
      "Epoch   231: reducing learning rate of group 0 to 2.5802e-08.\n",
      "Epoch   239: reducing learning rate of group 0 to 5.1604e-09.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:54:52,410    INFO], [train.py:312 - train()], Model training stopped after 259 epochs.\n",
      "[2022-02-02 11:54:52,410    INFO], [train.py:313 - train()], Best testing loss: -15.0705\n",
      "\u001B[32m[I 2022-02-02 11:54:52,435]\u001B[0m Trial 48 finished with value: 0.004017808474600315 and parameters: {'lr': 0.002015799533654733, 'l2_regularization': 0.13099442367259703, 'dropout': 0.3639528737213279, 'batch_size': 0.4690091661626862}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n",
      "[2022-02-02 11:54:52,458    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    69: reducing learning rate of group 0 to 6.3999e-05.\n",
      "Epoch    80: reducing learning rate of group 0 to 1.2800e-05.\n",
      "Epoch   107: reducing learning rate of group 0 to 2.5599e-06.\n",
      "Epoch   128: reducing learning rate of group 0 to 5.1199e-07.\n",
      "Epoch   136: reducing learning rate of group 0 to 1.0240e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:55:16,808    INFO], [train.py:312 - train()], Model training stopped after 158 epochs.\n",
      "[2022-02-02 11:55:16,808    INFO], [train.py:313 - train()], Best testing loss: -2.3035\n",
      "\u001B[32m[I 2022-02-02 11:55:16,833]\u001B[0m Trial 49 finished with value: 0.005459416192024946 and parameters: {'lr': 0.00031999363127510486, 'l2_regularization': 0.27841425178572066, 'dropout': 0.17251806708202314, 'batch_size': 0.34823475022872863}. Best is trial 16 with value: 0.00357465585693717.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   144: reducing learning rate of group 0 to 2.0480e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": "('/Users/jackarmand/Documents/GitHub/quantile-net/qnet/experiments/concrete/study/train_config_02_02_11_55.yaml',\n '/Users/jackarmand/Documents/GitHub/quantile-net/qnet/experiments/concrete/study/optuna_study_02_02_11_55.sav')"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qnet.study import OptunaStudy\n",
    "from qnet import ROOT\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(os.path.join(ROOT, \"data/concrete.csv\"))\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "base_config_path = os.path.join(ROOT, \"experiments/base_config.yaml\")\n",
    "\n",
    "base_config = yaml.full_load(open(base_config_path))\n",
    "model_config = {\n",
    "    \"categorical_features\":[],\n",
    "    \"continuous_features\":df.columns.to_list()[:-1],\n",
    "    \"multi_label_categorical_features\":[],\n",
    "    \"target\":df.columns.to_list()[-1:],\n",
    "    \"yeo_transform\":False\n",
    "}\n",
    "\n",
    "save_dir = os.path.join(ROOT, \"experiments/concrete/study\")\n",
    "study = OptunaStudy(\n",
    "    df=train_df,\n",
    "    model_config=model_config,\n",
    "    base_config=base_config,\n",
    "    save_folder=save_dir,\n",
    "    n_trials=50,\n",
    "    optimize_model_architecture=False,\n",
    "    optimize_batch_size=True,\n",
    "    optimize_momentum=True,\n",
    "    optimize_mse=True,\n",
    ")\n",
    "\n",
    "study.run_study()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:23:27,106    INFO], [models.py:38 - __init__()], Model on device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   154: reducing learning rate of group 0 to 6.0000e-05.\n",
      "Epoch   170: reducing learning rate of group 0 to 1.2000e-05.\n",
      "Epoch   193: reducing learning rate of group 0 to 2.4000e-06.\n",
      "Epoch   202: reducing learning rate of group 0 to 4.8000e-07.\n",
      "Epoch   225: reducing learning rate of group 0 to 9.6000e-08.\n",
      "Epoch   233: reducing learning rate of group 0 to 1.9200e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:24:24,428    INFO], [train.py:312 - train()], Model training stopped after 255 epochs.\n",
      "[2022-02-02 11:24:24,428    INFO], [train.py:313 - train()], Best testing loss: -7.257\n",
      "[2022-02-02 11:24:24,435 WARNING], [train.py:369 - save()], Directory is not empty. Overwriting\n",
      "[2022-02-02 11:24:24,473    INFO], [models.py:38 - __init__()], Model on device: None\n",
      "[2022-02-02 11:24:24,484    INFO], [models.py:176 - load()], Loading device : cpu\n"
     ]
    }
   ],
   "source": [
    "from qnet.train import Engine\n",
    "\n",
    "train_config = yaml.full_load(open(r\"/Users/jackarmand/Documents/GitHub/quantile-net/qnet/experiments/concrete/study/train_config_02_02_10_04.yaml\"))\n",
    "train_config[\"batch_size\"] = 0.2\n",
    "train_config[\"lr\"] = 3e-4\n",
    "train_config[\"optimizer\"] = \"adam\"\n",
    "engine = Engine(\n",
    "    train_config,\n",
    "    model_config,\n",
    "    df_init=train_df\n",
    ")\n",
    "engine.train()\n",
    "dir = engine.save(os.path.join(ROOT, \"experiments/concrete/model\"), append_auto_dir=False)\n",
    "engine = Engine.load(dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-02-02 11:24:35,148    INFO], [models.py:38 - __init__()], Model on device: None\n"
     ]
    }
   ],
   "source": [
    "from qnet.predictions import Predictions\n",
    "\n",
    "dir = os.path.join(ROOT, \"experiments/concrete/model\")\n",
    "pred_maker = Predictions(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rats\n",
      "rats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackarmand/Documents/GitHub/quantile-net/venv/lib/python3.9/site-packages/numpy/core/_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([3.3256552])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qnet.probability import generate_cdf_function\n",
    "\n",
    "log_list = []\n",
    "for row in test_df.index:\n",
    "    qn_predictions = pred_maker.predict(pd.DataFrame(test_df.loc[row]).T)\n",
    "    spline, xmin, xmax = generate_cdf_function(pred_maker.model.config.quantiles, qn_predictions[:-1], la=1)\n",
    "    d_spline = spline.derivative()\n",
    "    likelihood = d_spline(test_df.loc[row][model_config[\"target\"]])\n",
    "    if likelihood<1e-15:\n",
    "        likelihood = 1e-15\n",
    "        print(\"rats\")\n",
    "    log_list.append(-np.log(likelihood))\n",
    "\n",
    "np.mean(log_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}